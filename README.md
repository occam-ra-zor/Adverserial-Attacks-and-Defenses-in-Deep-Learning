# Adversarial Attacks and Defenses  in Deep Learning

Following is the list of papers that I have read for understanding about Adverserial Attacks and Defenses. Along with the PyTorch Code I implemented (not Yet added).



 
## Papers


* [Intruiging Properties of Neural Networks](https://arxiv.org/pdf/1312.6199.pdf)
* [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)
* [DeepFool: a simple and accurate method to fool deep neural networks](https://arxiv.org/abs/1511.04599.pdf)
* [Adversarial Examples In Physical World](https://arxiv.org/pdf/1607.02533.pdf)
* [Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks](https://arxiv.org/pdf/1511.04508.pdf)
* [Towards Evaluting the Adversarial Robustness of Neural Networks](https://arxiv.org/pdf/1608.04644.pdf)
* [Towards Deep Learning Models Resistant to Adversarial Attacks](https://arxiv.org/pdf/1706.06083.pdf)
* [Adverserial Machine Learning At Scale](https://arxiv.org/pdf/1611.01236.pdf)
* [SoK: Towards the Science of Security and Privacy in Machine Learning](https://arxiv.org/pdf/1611.03814.pdf)
* [Ensemble Adverserial Training : Attack and Defense](https://arxiv.org/pdf/1705.07204.pdf)
* [Adversarial Examples In Physical World](https://arxiv.org/pdf/1607.02533.pdf)
* [Distributional Smoothing with Virtual Adversarial Training](https://arxiv.org/abs/1507.00677)
* [On Adaptive Attacks to Adversarial Example Defenses](https://arxiv.org/pdf/2002.08347.pdf)
* [Universal Adversarial Perturbations](https://arxiv.org/abs/1610.08401.pdf)
* [On Detecting Adversarial Perturbations](https://arxiv.org/pdf/1702.04267.pdf)
* [Detecting Adversarial Examples from artifacts](https://arxiv.org/pdf/1703.00410.pdf)
* [Obfuscated Gradients Give a False Sense of Security :Circumventing Defenses to Adversarial Examples](https://arxiv.org/pdf/1802.00420.pdf)
 

